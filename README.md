Mestre JosÃ©, segue o README focado **sÃ³** neste projeto RAG, jÃ¡ deixando claro que ele foi criado e deve rodar em modo desenvolvimento usando o **N8N-DevHub**. Copie e cole direto no seu `README.md`:

---

# RAG â€“ Upload & Q\&A com Base Vetorial (n8n) ğŸ“šâš™ï¸

Projeto RAG (Retrieval-Augmented Generation) para **upload de PDFs/CSVs** e **perguntas e respostas** sobre o conteÃºdo enviado, construÃ­do em **n8n** com LangChain Nodes, **Groq** (LLM) e **Ollama** (embeddings).

> **ObservaÃ§Ã£o:** Este projeto foi criado e Ã© recomendado rodar em **modo desenvolvimento** dentro do ambiente **\[N8N-DevHub]** â€” o ambiente de desenvolvimento n8n que automatiza venv, dependÃªncias, Docker e sincronizaÃ§Ã£o.

---

## âœ¨ O que este projeto faz

* Recebe arquivos via **Form Trigger** (`.pdf` e `.csv`).
* Carrega e processa documentos (binary â†’ documento).
* Gera **embeddings com Ollama** (`mxbai-embed-large`).
* Indexa em um **VectorStore in-memory** (para testes rÃ¡pidos).
* ExpÃµe um **Webhook POST** que:

  * Aciona um **Agente (LangChain Agent)** com a ferramenta `knowledge_base` (RAG).
  * Responde com **texto** no prÃ³prio webhook.

---

## ğŸ§© Arquitetura (nÃ³s principais)

* **Form Trigger** â€“ `Upload your file here`
  *Aceita upload `.pdf, .csv`.*
* **Default Data Loader** â€“ `documentDefaultDataLoader`
  *Converte binÃ¡rio em documento para indexaÃ§Ã£o.*
* **Embeddings Ollama** â€“ `mxbai-embed-large:latest`
  *Modelo local de embeddings via Ollama.*
* **VectorStore In-Memory**

  * `Insert Data to Store` (modo `insert`, `memoryKey: vector_store_key`)
  * `Query Data Tool` (modo `retrieve-as-tool`, `topK: 10`, toolName: `knowledge_base`)
* **Groq Chat Model** â€“ `openai/gpt-oss-20b`
  *LLM hospedado na Groq para geraÃ§Ã£o de respostas.*
* **AI Agent** â€“ `LangChain Agent`
  *Prompt com instruÃ§Ãµes estritas: consultar **sempre** a knowledge\_base e responder **apenas** o perguntado.*
* **Webhook (POST)** â€“ path: `6dc67ac6-5287-451b-a0a9-a2a44b6368c4`
  *Entrada de mensagens; a resposta sai em texto.*
* **Respond to Webhook** â€“ retorna `{{$json.output}}`
* **Simple Memory** â€“ `Buffer Window (contextWindowLength: 4)`
  *MemÃ³ria curta por sessÃ£o, chaveando por `body.message`.*

---

## ğŸ§± PrÃ©-requisitos

* **Docker** e **Docker Compose**
* **Python 3.8+**
* **Git**
* **N8N-DevHub** (ambiente de desenvolvimento n8n)

> Para funcionar tudo de maneira correta em **modo desenvolvimento**, **use o N8N-DevHub**.
> Na raiz vazia do projeto, vocÃª pode baixÃ¡-lo diretamente:

```bash
git clone https://github.com/josejfs/N8N-DevHub.git .
chmod +x *.sh
./n8n-dev.sh start
```

Isso cria venv, instala dependÃªncias, levanta Docker e deixa o n8n pronto.
Este projeto RAG foi **criado usando o N8N-DevHub**.

---

## ğŸš€ Como rodar este RAG no N8N-DevHub

1. **Suba o ambiente** (se ainda nÃ£o estiver rodando):

```bash
./n8n-dev.sh start
```

2. **Importe o workflow** RAG no n8n:

   * Abra `http://localhost:5678`
   * **Import** â†’ cole o JSON do workflow (este projeto)

3. **Configure credenciais**:

   * **Groq API** (`Groq account`) â€“ necessÃ¡rio para o node **Groq Chat Model**

     * Adicione sua **GROQ\_API\_KEY** nas credenciais do n8n.
   * **Ollama** â€“ necessÃ¡rio para **Embeddings Ollama**

     * Tenha o **Ollama** rodando local e o modelo `mxbai-embed-large` disponÃ­vel:

       ```bash
       ollama pull mxbai-embed-large
       ```
     * Ajuste a credencial **Ollama API** no n8n (URL padrÃ£o local, se aplicÃ¡vel).

4. **Habilite o webhook** (se usar modo teste sem â€œlistenâ€ global):

   * Coloque o workflow em **Active** ou use **Test Webhook**.

---

## ğŸ§ª Teste rÃ¡pido

### 1) Subir documento via Form Trigger

* Acesse o node **Form Trigger** â€œUpload your file hereâ€.
* FaÃ§a upload de **PDF/CSV**.

### 2) Perguntar via Webhook (POST)

* Endpoint (local):

  ```
  POST http://localhost:5678/webhook-test/6dc67ac6-5287-451b-a0a9-a2a44b6368c4
  ```
* Exemplo `curl`:

  ```bash
  curl -X POST \
    http://localhost:5678/webhook-test/6dc67ac6-5287-451b-a0a9-a2a44b6368c4 \
    -H "Content-Type: application/json" \
    -d '{"body":{"message":"qual Ã© o assunto do PDF?"}}'
  ```
* A resposta textual vem do **Respond to Webhook** com `{{$json.output}}`.

> Dica: o **AI Agent** usa o tool `knowledge_base` (VectorStore) para recuperar trechos relevantes e **responde apenas** o que foi perguntado, conforme o prompt.

---

## âš™ï¸ VariÃ¡veis/credenciais esperadas

* **GROQ\_API\_KEY** (credencial do Groq no n8n)
* **Ollama** ativo localmente e modelo `mxbai-embed-large` instalado

Se estiver rodando tudo dentro do **N8N-DevHub**, o `docker-compose` e os scripts jÃ¡ cuidam do ambiente do n8n; vocÃª sÃ³ precisa configurar as credenciais no painel do n8n.

---

## ğŸ“ Estrutura sugerida

```
.
â”œâ”€â”€ workflows/                # JSONs de workflows (sincronizados)
â”œâ”€â”€ n8n_data/                 # Dados do n8n (nÃ£o versionar)
â”œâ”€â”€ n8n-dev.sh                # Scripts do N8N-DevHub
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                 # Este arquivo
```

> **Git:** adicione `n8n_data/` ao `.gitignore` para evitar subir dados/sessÃµes:

```
n8n_data/
```

---

## ğŸ” SoluÃ§Ã£o de problemas

* **Webhook nÃ£o responde**

  * Confirme se o workflow estÃ¡ **Active** ou em **Test Webhook**.
  * Verifique os **logs**:

    ```bash
    ./n8n-dev.sh logs
    ./n8n-dev.sh logs n8n
    ```
* **Embeddings falhando**

  * Cheque se o **Ollama** estÃ¡ rodando e o modelo `mxbai-embed-large` estÃ¡ disponÃ­vel:

    ```bash
    ollama list
    ollama pull mxbai-embed-large
    ```
* **LLM (Groq) sem resposta**

  * Confirme a **GROQ\_API\_KEY** nas credenciais.
* **Conflitos de permissÃ£o (Linux)**

  ```bash
  sudo chown -R $USER:$USER n8n_data workflows
  chmod -R 755 n8n_data workflows
  ```

---

## ğŸ—’ï¸ ObservaÃ§Ãµes de desenvolvimento

* O **VectorStore** Ã© **in-memory** (excelente para desenvolvimento/teste).
  Para produÃ§Ã£o, considere um store persistente (ex.: Qdrant, Chroma com volume, etc.).
* O **prompt do Agente** forÃ§a respostas **enxutas e factuais**, usando sempre a `knowledge_base` primeiro.
* A **memÃ³ria** do chat Ã© curta (`contextWindowLength: 4`) e identifica a sessÃ£o por `body.message` (ajuste se desejar uma chave de sessÃ£o mais estÃ¡vel).

---

## ğŸ“œ LicenÃ§a

Uso livre para desenvolvimento e estudos. Ajuste conforme sua necessidade.

---

## ğŸ”— Ambiente de desenvolvimento recomendado

Este projeto foi criado usando o **N8N-DevHub**. Para garantir que tudo rode corretamente em **modo desenvolvimento** (venv, Docker, sincronizaÃ§Ã£o, etc.), use:

```bash
git clone https://github.com/josejfs/N8N-DevHub.git .
chmod +x *.sh
./n8n-dev.sh start
```

Pronto! Suba o workflow, configure Groq e Ollama, faÃ§a upload de um PDF/CSV e jÃ¡ pode perguntar ğŸ˜‰